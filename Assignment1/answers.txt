1) ===================================================================
alpha = 0.2 
Unique words = {murder, cars, map, treasure, love, conspiracy,
               robbery, crash, enemy, family, vendetta, betrayal}
|V| = 12

Prior probabilities:
    P(action) = 4/8 = 1/2
    P(drama) = 4/8 = 1/2

Conditional probabilities:
    P(w | c) = (count(w, c) + alpha) / (count(c) + alpha * |V|)
               (count(w, c) + 0.2) / (count(c) + 2.4)
            
    P(murder | action) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(murder | drama) = (2 + 0.2) / (16 + 2.4) = 0.119565
    P(betrayal | action) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(betrayal | drama) = (2 + 0.2) / (16 + 2.4) = 0.119565
    P(enemy | action) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(enemy | drama) = (2 + 0.2) / (16 + 2.4) = 0.119565
    P(conspiracy | action) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(conspiracy | drama) = (3 + 0.2) / (16 + 2.4) = 0.173913
    P(cars | action) = (3 + 0.2) / (16 + 2.4) = 0.173913
    P(cars | drama) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(treasure | action) = (3 + 0.2) / (16 + 2.4) = 0.173913
    P(treasure | drama) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(robbery | action) = (2 + 0.2) / (16 + 2.4) = 0.119565
    P(robbery | drama) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(crash | action) = (2 + 0.2) / (16 + 2.4) = 0.119565
    P(crash | drama) = (0 + 0.2) / (16 + 2.4) = 0.005435
    P(map | action) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(map | drama) = (0 + 0.2) / (16 + 2.4) = 0.005435
    P(love | action) = (0 + 0.2) / (16 + 2.4) = 0.005435
    P(love | drama) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(vendetta | action) = (0 + 0.2) / (16 + 2.4) = 0.005435
    P(vendetta | drama) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(family | action) = (1 + 0.2) / (16 + 2.4) = 0.065217
    P(family | drama) = (2 + 0.2) / (16 + 2.4) = 0.119565

Posterior Probabilities:
    P(action | D1) = P(action) * P(murder | action) * P(betrayal | action)
                        * P(enemy | action) * P(conspiracy | action)
                   = 0.5 * 0.065217 * 0.065217 * 0.065217 * 0.065217
                   = 9.045 * 10^(-6)

    P(drama | D1) = P(drama) * P(murder | drama) * P(betrayal | drama)
                        * P(enemy | drama) * P(conspiracy | drama)
                  = 0.5 * 0.119565 * 0.119565 * 0.119565 * 0.173913
                  = 1.486 * 10^(-4)

    P(action | D2) = P(action) * P(cars | action) * P(treasure | action)
                        * P(robbery | action) * P(crash | action)
                   = 0.5 * 0.173913 * 0.173913 * 0.119565 * 0.119565
                   = 2.162 * 10^(-4)
    P(drama | D2) = P(drama) * P(cars | drama) * P(treasure | drama)
                        * P(robbery | drama) * P(crash | drama)
                  = 0.5 * 0.065217 * 0.065217 * 0.065217 * 0.005435
                  = 7.538 * 10^(-7)

The most likely class for D1 is drama and the most likely class for D2 is action.

2) ===================================================================
All code is located in Problem2/problem2.py

2.1)
    1. [The, DT, company, NN, of, IN, directors, NNS]
    2. [Several, JJ, loose, JJ, creaked, VBD, as, IN]
    3. [We, PRP, all, PRP, the, DT, plane, NN]
    4. [nailed, VBD, the, DT, over, IN, the, DT]
    5. [wanted, VBD, to, TO, the, DT, bus, NN]
    6. [senators, NNS, on, RP, for, IN, the, DT]

2.2.1) Example feature vector:
[['38201', 'to_activate', 'on_to_activate', 'activate_it', 'activate_it_.', 1, 0, 0, 0, 0, 0, 0, 2 ... ]
The first value is the target, the sense of the word.
The next four values are the previous and next bigrams and trigrams of the word in question.
The next value is a 1 or a 0 representing whether the word in question appears near the end of the sentence (my custom feature).
The following values are the count of words from the Bag-of-Words in the sample sentence.

2.2.2) Using f_classif (ANOVA F-test), top 10 labels were:
['BoW count for word: turgor', 'BoW count for word: relax',
 'BoW count for word: sharply', 'BoW count for word: nostalgic',
 'BoW count for word: gladiatorial', 'BoW count for word: gains',
 'BoW count for word: gritty', 'BoW count for word: crest',
 'BoW count for word: 17.29', 'BoW count for word: Jooss']

 I chose the F-Test because it's suitable for classification feature selection. It compares models
 with and without the features by running an F-Test and thus chooses which features are more important.

3) ========================================================================

3.4) 
Linear interpolation had the benefit of identifying the proper lambda values using the hold-out set, whereas add-alpha smoothing
seemed to be dependent on the right alpha value being chosen ahead of time.

On the other hand, because linear interpolation needs a hold-out set to identify lambda values, add-alpha smoothing was able to train on a larger
data set, which may make it a more accurate model in the end, if as we said the right alpha value is chosen.
